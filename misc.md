# Miscellaneous topics


### Contents

1.  [LoRA](#lora)
2.  [RAG](#rag)
3.  [Voice to text](#voice-to-text)
4.  [Multimodal models](#multimodal-models)
5.  [More](#more)


## LoRA

-    LoRA: Low-rank adaptation

![LoRA. source: https://arxiv.org/abs/2106.09685](img/lora.png)

-   Hu, E.J. et al (2021). [LoRA: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685)
-   Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). [QLoRA: Efficient finetuning of quantized LLMs](https://arxiv.org/abs/2305.14314).
-   Zhao, J. et al (2024). [GaLore: Memory-efficient LLM training by gradient low-rank projection](https://arxiv.org/abs/2403.03507).


## RAG

-   Retrieval-Augmented Generation (RAG)
-   Requires a large context size to hold the retrieved documents

![RAG explained.](img/rag-explained-clarifai.png)


## Voice to text

-   Whisper model (OpenAI)
-   Transformer based, encoder-decoder

![Whisper model (source: [openai](https://openai.com/research/whisper)).](img/whisper.png)


## Multimodal models

![Figure from [Gemini](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf).](img/gemini-multimodal.png)


## More

-   Yue, X. et al. (2023).  [MMMU: A Massive Multi-discipline Multimodal Understanding and reasoning benchmark for expert AGI](https://arxiv.org/abs/2311.16502).
-   [Insights from Kaggle Grandmasters and Experts on Competitive AI and LLM Frontiers](https://www.youtube.com/watch?v=k2EcIX0HgzA). *GTC, 2024*.


--------

-   Up next: TODO
-   Previous: [Parallelism and hardware](parallelism-and-hw.md)

