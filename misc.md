# Miscellaneous topics


### Contents

1.  [LoRA](#lora)
2.  [RAG](#rag)


## LoRA

![LoRA. source: https://arxiv.org/abs/2106.09685](img/lora.png)

-   Hu, E.J. et al (2021). [LoRA: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685)
-   Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). [QLoRA: Efficient finetuning of quantized LLMs](https://arxiv.org/abs/2305.14314).
-   Zhao, J. et al (2024). [GaLore: Memory-efficient LLM training by gradient low-rank projection](https://arxiv.org/abs/2403.03507).


## RAG

TODO

